{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances, accuracy_score, confusion_matrix, classification_report\n",
    "import warnings, os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "base_path = \"/kaggle/input\"\n",
    "folders = os.listdir(base_path)\n",
    "\n",
    "csv_path = None\n",
    "for folder in folders:\n",
    "    if \"imdb\" in folder.lower():\n",
    "        potential_path = f\"{base_path}/{folder}/imdb_movies_main.csv\"\n",
    "        if os.path.exists(potential_path):\n",
    "            csv_path = potential_path\n",
    "            break\n",
    "\n",
    "if csv_path is None:\n",
    "    csv_path = f\"{base_path}/{folders[0]}/imdb_movies_main.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(\"Loaded dataset shape:\", df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2 — preprocessing\n",
    "\n",
    "\n",
    "\n",
    "# A. Date parsing (your dataset uses 'date_x')\n",
    "if 'date_x' in df.columns:\n",
    "    df['release_date'] = pd.to_datetime(df['date_x'], errors='coerce')\n",
    "    df['release_year'] = df['release_date'].dt.year\n",
    "else:\n",
    "    df['release_year'] = np.nan\n",
    "\n",
    "# B. Convert numeric columns (based on actual dataset)\n",
    "numeric_cols_present = [\n",
    "    col for col in ['runtime_min','runtime','budget_x','revenue','popularity',\n",
    "                    'score','vote_count']\n",
    "    if col in df.columns\n",
    "]\n",
    "\n",
    "for col in numeric_cols_present:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# C. Create final “rating” column (your dataset uses “score”)\n",
    "if 'score' in df.columns:\n",
    "    df['rating'] = df['score']\n",
    "elif 'vote_average' in df.columns:\n",
    "    df['rating'] = df['vote_average']\n",
    "else:\n",
    "    df['rating'] = np.nan\n",
    "\n",
    "# D. Genres parsing (your dataset uses “genre”)\n",
    "if 'genre' in df.columns:\n",
    "    df['genres_list'] = df['genre'].astype(str).apply(\n",
    "        lambda x: [g.strip().lower() for g in x.split(',')] \n",
    "        if pd.notna(x) and x.strip() != '' else []\n",
    "    )\n",
    "else:\n",
    "    df['genres_list'] = [[] for _ in range(len(df))]\n",
    "\n",
    "# E. Director & Actor extraction\n",
    "if 'crew' in df.columns:\n",
    "    df['director_clean'] = df['crew'].astype(str).apply(\n",
    "        lambda x: x.split(',')[0].strip() if pd.notna(x) and ',' in x else np.nan\n",
    "    )\n",
    "    df['actors_list'] = df['crew'].astype(str).apply(\n",
    "        lambda x: [p.strip() for p in x.split(',')[1:4]]\n",
    "        if pd.notna(x) and ',' in x else []\n",
    "    )\n",
    "else:\n",
    "    df['director_clean'] = np.nan\n",
    "    df['actors_list'] = [[] for _ in range(len(df))]\n",
    "\n",
    "# F. Missing value reporting\n",
    "missing_report = df.isnull().sum().sort_values(ascending=False)\n",
    "print(\"Top missing columns:\\n\", missing_report.head(10))\n",
    "\n",
    "# Fill numeric missing with median\n",
    "numeric_final = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_final] = df[numeric_final].fillna(df[numeric_final].median())\n",
    "\n",
    "# Fill categorical missing with 'unknown'\n",
    "categorical_final = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_final] = df[categorical_final].fillna('unknown')\n",
    "\n",
    "# G. Outlier Detection using IQR (budget_x, revenue, popularity)\n",
    "outlier_flags = pd.DataFrame(index=df.index)\n",
    "\n",
    "for col in ['budget_x','revenue','vote_count','popularity','runtime_min']:\n",
    "    if col in df.columns:\n",
    "        series = df[col]\n",
    "        Q1, Q3 = series.quantile(0.25), series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "        outlier_flags[col + '_outlier'] = ~series.between(lower, upper)\n",
    "\n",
    "df = pd.concat([df, outlier_flags], axis=1)\n",
    "\n",
    "# H. Normalization (MinMax scaling)\n",
    "scale_features = []\n",
    "for col in ['runtime_min','budget_x','revenue','popularity','vote_count','rating']:\n",
    "    if col in df.columns:\n",
    "        scale_features.append(col)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[[c + '_scaled' for c in scale_features]] = scaler.fit_transform(df[scale_features])\n",
    "\n",
    "# I. Discretization (ordinal) for rating\n",
    "if 'rating' in df.columns:\n",
    "    df['rating_bin'] = pd.qcut(df['rating'], 4, labels=['low','mid','high','very_high'])\n",
    "\n",
    "# J. PCA (only on scaled numeric columns)\n",
    "pca_cols = [c + '_scaled' for c in scale_features]\n",
    "\n",
    "if len(pca_cols) >= 2:\n",
    "    pca = PCA(n_components=min(3, len(pca_cols)))\n",
    "    pca_res = pca.fit_transform(df[pca_cols])\n",
    "    for i in range(pca_res.shape[1]):\n",
    "        df[f'pca_{i+1}'] = pca_res[:, i]\n",
    "    print(\"PCA variance:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# K. Sampling for heavy processing later (limit 5000)\n",
    "df_sample = df.sample(n=min(5000, len(df)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Preprocessing complete. Sample shape:\", df_sample.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Basic Graphs & EDA\n",
    "\n",
    "# A. Rating Distribution\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df['rating'].dropna(), bins=30, kde=True)\n",
    "plt.title(\"IMDb Rating Distribution\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# B. Genre Popularity Over Decades (Corrected)\n",
    "# ============================\n",
    "\n",
    "# Safely create release_year\n",
    "if 'date_x' in df.columns:\n",
    "    df['release_year'] = pd.to_datetime(df['date_x'], errors='coerce').dt.year\n",
    "elif 'release_date' in df.columns:\n",
    "    df['release_year'] = pd.to_datetime(df['release_date'], errors='coerce').dt.year\n",
    "elif 'year' in df.columns:\n",
    "    df['release_year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "else:\n",
    "    df['release_year'] = np.nan\n",
    "\n",
    "# Clean year\n",
    "df_year = df.dropna(subset=['release_year']).copy()\n",
    "df_year['release_year'] = df_year['release_year'].astype(int)\n",
    "\n",
    "# Explode genres safely\n",
    "df_exp = df_year.explode('genres_list')\n",
    "df_exp = df_exp[df_exp['genres_list'].notna() & (df_exp['genres_list'] != '')]\n",
    "\n",
    "# Create decade\n",
    "df_exp['decade'] = (df_exp['release_year'] // 10) * 10\n",
    "\n",
    "# OPTIONAL FIX: Merge incomplete 2020 decade into 2010\n",
    "df_exp['decade'] = df_exp['decade'].replace({2020: 2010})\n",
    "\n",
    "# Select top genres\n",
    "top_genres = df_exp['genres_list'].value_counts().head(8).index\n",
    "\n",
    "# Trend data\n",
    "df_trend = df_exp[df_exp['genres_list'].isin(top_genres)]\n",
    "trend = df_trend.groupby(['decade','genres_list']).size().reset_index(name='count')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.lineplot(data=trend, x='decade', y='count', hue='genres_list', marker='o')\n",
    "plt.title(\"Genre Popularity Over Decades (Corrected)\")\n",
    "plt.xlabel(\"Decade\")\n",
    "plt.ylabel(\"Number of Movies\")\n",
    "plt.legend(title=\"Genre\", bbox_to_anchor=(1.05,1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# C. Top Genres Bar Chart\n",
    "# ============================\n",
    "\n",
    "df_gen = df.explode('genres_list')\n",
    "df_gen = df_gen[df_gen['genres_list'].notna() & (df_gen['genres_list'] != '')]\n",
    "\n",
    "top_gen = df_gen['genres_list'].value_counts().head(12)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "sns.barplot(x=top_gen.values, y=top_gen.index)\n",
    "plt.title(\"Top Genres\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Genre\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# D. Runtime vs Rating Scatter\n",
    "# ============================\n",
    "if 'runtime_min' in df.columns:\n",
    "    df_rt = df.dropna(subset=['runtime_min', 'rating'])\n",
    "    plt.figure(figsize=(7,4))\n",
    "    sns.scatterplot(data=df_rt, x='runtime_min', y='rating', alpha=0.6)\n",
    "    plt.title(\"Runtime vs Rating\")\n",
    "    plt.xlabel(\"Runtime (min)\")\n",
    "    plt.ylabel(\"Rating\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# E. Budget vs Rating Scatter\n",
    "# ============================\n",
    "\n",
    "budget_col = next((c for c in ['budget', 'budget_x', 'budget_adj', 'budget_in_usd'] if c in df.columns), None)\n",
    "\n",
    "if budget_col:\n",
    "    df_b = df.dropna(subset=[budget_col, 'rating'])\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.scatterplot(x=df_b[budget_col], y=df_b['rating'], alpha=0.6)\n",
    "    plt.title(f\"Budget vs Rating ({budget_col})\")\n",
    "    plt.xlabel(budget_col)\n",
    "    plt.ylabel(\"Rating\")\n",
    "    plt.xscale(\"symlog\")  # handles large movie budgets\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# F. Top Directors by Avg Rating\n",
    "# ============================\n",
    "\n",
    "if 'director_clean' in df.columns:\n",
    "    df_dir = df[df['director_clean'].notna() & (df['director_clean'] != 'unknown')]\n",
    "    dir_avg = df_dir.groupby('director_clean')['rating'].mean().sort_values(ascending=False).head(12)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=dir_avg.values, y=dir_avg.index)\n",
    "    plt.title(\"Top Directors by Average Rating\")\n",
    "    plt.xlabel(\"Average Rating\")\n",
    "    plt.ylabel(\"Director\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# G. Top Actors by Movie Count\n",
    "# ============================\n",
    "\n",
    "actors_ex = df.explode('actors_list')\n",
    "actors_ex = actors_ex[actors_ex['actors_list'].notna() & (actors_ex['actors_list'] != '')]\n",
    "\n",
    "actor_counts = actors_ex['actors_list'].value_counts().head(12)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=actor_counts.values, y=actor_counts.index)\n",
    "plt.title(\"Top Actors by Movie Count\")\n",
    "plt.xlabel(\"Movie Count\")\n",
    "plt.ylabel(\"Actor\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# H. Correlation Heatmap of Scaled Features\n",
    "# ============================\n",
    "\n",
    "scaled_cols = [c for c in df.columns if c.endswith('_scaled')]\n",
    "if len(scaled_cols) >= 2:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(df[scaled_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(\"Correlation Heatmap (Scaled Features)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# CELL 4 — Similarity & Dissimilarity (Fully Corrected)\n",
    "# ================================================================\n",
    "\n",
    "sample = df_sample.copy()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. NUMERIC FEATURES FOR SIMILARITY (scaled)\n",
    "# ------------------------------------------------\n",
    "num_cols = []\n",
    "for col in ['runtime_min', 'budget_x', 'revenue', 'popularity', 'vote_count', 'rating']:\n",
    "    c_scaled = col + \"_scaled\"\n",
    "    if c_scaled in sample.columns:\n",
    "        num_cols.append(c_scaled)\n",
    "\n",
    "if num_cols:\n",
    "    X_num = sample[num_cols].fillna(0).to_numpy()\n",
    "else:\n",
    "    X_num = np.zeros((len(sample), 0))\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. GENRE BINARY ENCODING (top 20 genres)\n",
    "# ------------------------------------------------\n",
    "all_genres = Counter([g for lst in sample['genres_list'] for g in lst])\n",
    "top_genres = [g for g, _ in all_genres.most_common(20)]\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=top_genres)\n",
    "genre_bin = mlb.fit_transform(sample['genres_list'])\n",
    "\n",
    "# Final similarity matrix input\n",
    "X_sim = np.hstack([X_num, genre_bin]) if X_num.size else genre_bin\n",
    "\n",
    "print(\"Numeric + Genre Matrix Shape:\", X_sim.shape)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. Pairwise Euclidean, Manhattan, Cosine\n",
    "# ------------------------------------------------\n",
    "n_show = min(6, X_sim.shape[0])  # show first few rows only\n",
    "\n",
    "pair_eu = pairwise_distances(X_sim[:n_show], metric=\"euclidean\")\n",
    "pair_man = pairwise_distances(X_sim[:n_show], metric=\"manhattan\")\n",
    "pair_cos = 1 - pairwise_distances(X_sim[:n_show], metric=\"cosine\")  # similarity\n",
    "\n",
    "print(\"\\nEuclidean Distance:\\n\", np.round(pair_eu, 3))\n",
    "print(\"\\nManhattan Distance:\\n\", np.round(pair_man, 3))\n",
    "print(\"\\nCosine Similarity:\\n\", np.round(pair_cos, 3))\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4. Jaccard & SMC (on genre binary only)\n",
    "# ------------------------------------------------\n",
    "def jaccard_matrix(M):\n",
    "    n = M.shape[0]\n",
    "    J = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            inter = np.logical_and(M[i], M[j]).sum()\n",
    "            union = np.logical_or(M[i], M[j]).sum()\n",
    "            J[i, j] = inter / union if union > 0 else 0.0\n",
    "    return J\n",
    "\n",
    "def smc_matrix(M):\n",
    "    n = M.shape[0]\n",
    "    S = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            S[i, j] = (M[i] == M[j]).sum() / len(M[i])\n",
    "    return S\n",
    "\n",
    "if genre_bin.size:\n",
    "    print(\"\\nJaccard Similarity (Genres):\\n\", np.round(jaccard_matrix(genre_bin[:n_show]), 3))\n",
    "    print(\"\\nSMC (Genres):\\n\", np.round(smc_matrix(genre_bin[:n_show]), 3))\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5. Set Difference (Example between first 2 movies)\n",
    "# ------------------------------------------------\n",
    "if len(sample) >= 2:\n",
    "    g0 = set(sample.iloc[0]['genres_list'])\n",
    "    g1 = set(sample.iloc[1]['genres_list'])\n",
    "    print(\"\\nSet Difference (Movie0 - Movie1):\", g0 - g1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# CELL 5 — Classification (Corrected & Dataset-Safe)\n",
    "# ================================================================\n",
    "\n",
    "clf_sample = df_sample.copy()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. Feature Construction (numeric_scaled + genre binary)\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Numeric scaled columns from Cell 2\n",
    "feat_num = [c for c in clf_sample.columns if c.endswith(\"_scaled\")]\n",
    "\n",
    "\n",
    "top_gen = list(all_genres.keys())[:20] \n",
    "mlb = MultiLabelBinarizer(classes=top_gen)\n",
    "genre_bin = mlb.fit_transform(clf_sample[\"genres_list\"])\n",
    "\n",
    "# Add genre columns\n",
    "for i, g in enumerate(top_gen):\n",
    "    clf_sample[f\"genre_{g}\"] = genre_bin[:, i]\n",
    "\n",
    "feat_cols = feat_num + [f\"genre_{g}\" for g in top_gen]\n",
    "\n",
    "print(\"Total feature columns:\", len(feat_cols))\n",
    "\n",
    "\n",
    "X = clf_sample[feat_cols].fillna(0)\n",
    "\n",
    "# Convert rating_bin to numeric labels 0–3\n",
    "y = clf_sample[\"rating_bin\"].astype(\"category\").cat.codes\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights=\"distance\")\n",
    "knn_cv_scores = cross_val_score(knn, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "print(\"\\n===== KNN Weighted Classification =====\")\n",
    "print(\"KNN CV Mean Accuracy:\", knn_cv_scores.mean())\n",
    "print(\"KNN Test Accuracy:\", accuracy_score(y_test, y_pred_knn))\n",
    "print(\"KNN Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_knn))\n",
    "\n",
    "\n",
    "for crit in [\"gini\", \"entropy\"]:\n",
    "    dt = DecisionTreeClassifier(\n",
    "        criterion=crit,\n",
    "        max_depth=6,\n",
    "        min_samples_leaf=10,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    dt_cv_scores = cross_val_score(dt, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    dt.fit(X_train, y_train)\n",
    "    preds_dt = dt.predict(X_test)\n",
    "\n",
    "    print(f\"\\n===== Decision Tree ({crit.upper()}) =====\")\n",
    "    print(\"CV Mean Accuracy:\", dt_cv_scores.mean())\n",
    "    print(\"Test Accuracy:\", accuracy_score(y_test, preds_dt))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds_dt))\n",
    "\n",
    "\n",
    "dt_deep = DecisionTreeClassifier(random_state=42)\n",
    "dt_deep.fit(X_train, y_train)\n",
    "\n",
    "train_acc = accuracy_score(y_train, dt_deep.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, dt_deep.predict(X_test))\n",
    "\n",
    "print(\"\\n===== Overfitting Check =====\")\n",
    "print(\"Deep Tree Train Accuracy:\", train_acc)\n",
    "print(\"Deep Tree Test Accuracy:\", test_acc)\n",
    "print(\"(Large difference = Overfitting)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# CELL 6 — Association Rule Mining (Apriori)\n",
    "# ================================================================\n",
    "\n",
    "transactions = [tuple(sorted(set(g))) for g in df_sample[\"genres_list\"] if g]\n",
    "\n",
    "def apriori(transactions, min_support=0.02):\n",
    "    n = len(transactions)\n",
    "    L = {}\n",
    "\n",
    "    # C1 candidate 1-itemsets\n",
    "    item_counts = Counter(itertools.chain.from_iterable(transactions))\n",
    "    L1 = {(i,): c/n for i, c in item_counts.items() if c/n >= min_support}\n",
    "    L.update(L1)\n",
    "\n",
    "    current_L = set(L1.keys())\n",
    "    k = 1\n",
    "\n",
    "    while current_L:\n",
    "        candidates = set()\n",
    "        curr = sorted(current_L)\n",
    "\n",
    "        # join step\n",
    "        for a in curr:\n",
    "            for b in curr:\n",
    "                union = tuple(sorted(set(a) | set(b)))\n",
    "                if len(union) == k + 1:\n",
    "                    candidates.add(union)\n",
    "\n",
    "        # count support\n",
    "        cand_support = {}\n",
    "        for cand in candidates:\n",
    "            cnt = sum(1 for t in transactions if set(cand).issubset(t))\n",
    "            sup = cnt / n\n",
    "            if sup >= min_support:\n",
    "                cand_support[cand] = sup\n",
    "\n",
    "        L.update(cand_support)\n",
    "        current_L = set(cand_support.keys())\n",
    "        k += 1\n",
    "\n",
    "    return L\n",
    "\n",
    "\n",
    "# Run apriori\n",
    "freq_itemsets = apriori(transactions, min_support=0.02)\n",
    "\n",
    "freq_df = (\n",
    "    pd.DataFrame([(k, v) for k, v in freq_itemsets.items()],\n",
    "                 columns=[\"itemset\", \"support\"])\n",
    "    .sort_values(\"support\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Frequent itemsets:\")\n",
    "display(freq_df.head(20))\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Create Association Rules\n",
    "# =====================\n",
    "def generate_rules(freq_itemsets, transactions, min_conf=0.3):\n",
    "    n = len(transactions)\n",
    "    rules = []\n",
    "\n",
    "    for itemset in freq_itemsets:\n",
    "        if len(itemset) < 2:\n",
    "            continue\n",
    "\n",
    "        sup_item = freq_itemsets[itemset]\n",
    "\n",
    "        # generate all proper subsets\n",
    "        subsets = [\n",
    "            tuple(sorted(s))\n",
    "            for i in range(1, len(itemset))\n",
    "            for s in itertools.combinations(itemset, i)\n",
    "        ]\n",
    "\n",
    "        for A in subsets:\n",
    "            B = tuple(sorted(set(itemset) - set(A)))\n",
    "            sup_A = freq_itemsets.get(A, sum(\n",
    "                1 for t in transactions if set(A).issubset(t)) / n)\n",
    "            if sup_A == 0:\n",
    "                continue\n",
    "\n",
    "            conf = sup_item / sup_A\n",
    "\n",
    "            sup_B = freq_itemsets.get(B, sum(\n",
    "                1 for t in transactions if set(B).issubset(t)) / n)\n",
    "\n",
    "            lift = conf / sup_B if sup_B else np.nan\n",
    "\n",
    "            if conf >= min_conf:\n",
    "                rules.append({\n",
    "                    \"antecedent\": A,\n",
    "                    \"consequent\": B,\n",
    "                    \"support\": sup_item,\n",
    "                    \"confidence\": conf,\n",
    "                    \"lift\": lift\n",
    "                })\n",
    "\n",
    "    if rules:\n",
    "        return pd.DataFrame(rules).sort_values([\"lift\", \"confidence\"], ascending=False)\n",
    "    return pd.DataFrame([])\n",
    "\n",
    "\n",
    "rules_df = generate_rules(freq_itemsets, transactions, min_conf=0.3)\n",
    "\n",
    "print(\"Top Association Rules:\")\n",
    "display(rules_df.head(20))\n",
    "\n",
    "\n",
    "# Closed and maximal itemsets\n",
    "closed = []\n",
    "maximal = []\n",
    "\n",
    "for s in freq_itemsets:\n",
    "    is_closed = True\n",
    "    is_max = True\n",
    "    for t in freq_itemsets:\n",
    "        if set(s).issubset(set(t)) and s != t:\n",
    "            if freq_itemsets[t] == freq_itemsets[s]:\n",
    "                is_closed = False\n",
    "            is_max = False\n",
    "    if is_closed:\n",
    "        closed.append(s)\n",
    "    if is_max:\n",
    "        maximal.append(s)\n",
    "\n",
    "print(\"Closed frequent sets (sample):\", closed[:10])\n",
    "print(\"Maximal frequent sets (sample):\", maximal[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# CELL 7 — Clustering: KMeans (Elbow, Silhouette) + DBSCAN\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "cluster_num_cols = [c for c in df_sample.columns if c.endswith(\"_scaled\")]\n",
    "\n",
    "\n",
    "top_gen_cluster = list(all_genres.keys())[:20]\n",
    "mlb_cluster = MultiLabelBinarizer(classes=top_gen_cluster)\n",
    "genre_bin_cluster = mlb_cluster.fit_transform(df_sample[\"genres_list\"])\n",
    "\n",
    "\n",
    "X_cluster = np.hstack([\n",
    "    df_sample[cluster_num_cols].fillna(0).to_numpy(),\n",
    "    genre_bin_cluster\n",
    "])\n",
    "\n",
    "print(\"Clustering matrix shape:\", X_cluster.shape)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. Elbow Method for KMeans\n",
    "# ---------------------------------------------------------------\n",
    "inertia = []\n",
    "K_range = range(2, 9)\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X_cluster)\n",
    "    inertia.append(km.inertia_)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(list(K_range), inertia, marker=\"o\")\n",
    "plt.title(\"Elbow Method for KMeans\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Silhouette Score for a chosen k\n",
    "# ---------------------------------------------------------------\n",
    "k_choice = 4 if X_cluster.shape[0] >= 4 else 2\n",
    "\n",
    "km = KMeans(n_clusters=k_choice, random_state=42, n_init=10)\n",
    "labels_km = km.fit_predict(X_cluster)\n",
    "\n",
    "if len(set(labels_km)) > 1:  # silhouette requires at least 2 clusters\n",
    "    sil = silhouette_score(X_cluster, labels_km)\n",
    "    print(f\"Silhouette Score (k={k_choice}):\", sil)\n",
    "else:\n",
    "    print(\"Silhouette cannot be computed (only one cluster found).\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. DBSCAN clustering\n",
    "# ---------------------------------------------------------------\n",
    "db = DBSCAN(eps=0.5, min_samples=5).fit(X_cluster)\n",
    "labels_db = db.labels_\n",
    "\n",
    "n_clusters = len(set(labels_db)) - (1 if -1 in labels_db else 0)\n",
    "n_noise = list(labels_db).count(-1)\n",
    "\n",
    "print(f\"DBSCAN: {n_clusters} clusters found\")\n",
    "print(f\"DBSCAN: {n_noise} noise points detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: Anomaly Detection (3-sigma, KNN proximity-based, DBSCAN noise). Evaluation template (confusion matrix)\n",
    "\n",
    "\n",
    "an_col = 'revenue' if 'revenue' in df.columns else ('budget' if 'budget' in df.columns else None)\n",
    "if an_col:\n",
    "    s = pd.to_numeric(df[an_col].fillna(0))\n",
    "    mu, sigma = s.mean(), s.std()\n",
    "    ultra_upper = mu + 3*sigma; ultra_lower = mu - 3*sigma\n",
    "    an3_idx = df[(s>ultra_upper) | (s<ultra_lower)].index\n",
    "    print(f\"3-sigma anomalies on {an_col}: {len(an3_idx)} rows (sample):\")\n",
    "    display(df.loc[an3_idx, ['names', an_col]].head())\n",
    "\n",
    "# Proximity-based: KNN mean distance as anomaly score (use X_cluster)\n",
    "nbrs = NearestNeighbors(n_neighbors=5).fit(X_cluster)\n",
    "dists, idxs = nbrs.kneighbors(X_cluster)\n",
    "anom_scores = dists.mean(axis=1)\n",
    "threshold = np.percentile(anom_scores, 95)\n",
    "knn_anom_idx = np.where(anom_scores > threshold)[0]\n",
    "print(\"KNN proximity anomalies (top 5 sample count):\", len(knn_anom_idx))\n",
    "print(\"Sample anomaly scores (top 5 indices):\", knn_anom_idx[:5])\n",
    "\n",
    "# Clustering-based: DBSCAN noise (labels_db from clustering cell)\n",
    "if 'labels_db' in globals():\n",
    "    db_noise_idx = np.where(labels_db == -1)[0]\n",
    "    print(\"DBSCAN noise points count:\", len(db_noise_idx))\n",
    "\n",
    "# Evaluation note: if ground-truth anomaly labels existed (y_true binary), compute:\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix(y_true, y_pred); print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Director helper (dataset-safe) + save cleaned datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "def show_top3_director(director_name, df_full=df):\n",
    "    # Normalize director column and input\n",
    "    if 'director_clean' not in df_full.columns:\n",
    "        print(\"No 'director_clean' column in dataset.\")\n",
    "        return\n",
    "\n",
    "    director_mask = df_full['director_clean'].astype(str).str.strip().str.lower() == director_name.strip().lower()\n",
    "    sub = df_full[director_mask].copy()\n",
    "\n",
    "    if sub.empty:\n",
    "        print(\"No movies for director:\", director_name)\n",
    "        return\n",
    "\n",
    "    # Choose a release column to display\n",
    "    if 'release_year' in sub.columns:\n",
    "        release_col = 'release_year'\n",
    "    elif 'release_date' in sub.columns:\n",
    "        release_col = 'release_date'\n",
    "    elif 'date_x' in sub.columns:\n",
    "        release_col = 'date_x'\n",
    "    elif 'year' in sub.columns:\n",
    "        release_col = 'year'\n",
    "    else:\n",
    "        release_col = None\n",
    "\n",
    "    # Prefer runtime_min, else runtime, else try to extract numeric runtime\n",
    "    if 'runtime_min' in sub.columns:\n",
    "        runtime_col = 'runtime_min'\n",
    "    elif 'runtime' in sub.columns:\n",
    "        runtime_col = 'runtime'\n",
    "    else:\n",
    "        runtime_col = None\n",
    "\n",
    "    # Ensure rating exists\n",
    "    if 'rating' not in sub.columns:\n",
    "        print(\"No 'rating' column found to sort by.\")\n",
    "        return\n",
    "\n",
    "    # Sort by rating (desc) then by release_year (desc) for tie-break\n",
    "    sort_cols = ['rating']\n",
    "    if release_col:\n",
    "        sort_cols.append(release_col)\n",
    "    sub = sub.sort_values(by=sort_cols, ascending=[False] * len(sort_cols))\n",
    "\n",
    "    top3 = sub.head(3).copy()\n",
    "\n",
    "    # Prepare display columns (safe picks)\n",
    "    display_cols = []\n",
    "    if 'names' in top3.columns:\n",
    "        display_cols.append('names')\n",
    "    if release_col:\n",
    "        display_cols.append(release_col)\n",
    "    display_cols.append('rating')\n",
    "    if runtime_col:\n",
    "        display_cols.append(runtime_col)\n",
    "\n",
    "    print(f\"Top {len(top3)} movies for director: {director_name}\")\n",
    "    display(top3[display_cols])\n",
    "\n",
    "    # Plot: bar for rating, line for runtime (if runtime exists)\n",
    "    fig, ax1 = plt.subplots(figsize=(9,5))\n",
    "    names = top3['names'].astype(str).tolist()\n",
    "\n",
    "    # Bar: ratings\n",
    "    ax1.bar(names, top3['rating'].astype(float), alpha=0.7)\n",
    "    ax1.set_ylabel('Rating')\n",
    "    ax1.set_ylim(0, max(10, top3['rating'].max() * 1.1))  # assume rating scale up to 10\n",
    "\n",
    "    # Line: runtime on secondary axis if available and numeric\n",
    "    if runtime_col:\n",
    "        try:\n",
    "            runtimes = pd.to_numeric(top3[runtime_col], errors='coerce')\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(names, runtimes, color='C1', marker='o', linewidth=2)\n",
    "            ax2.set_ylabel('Runtime (min)')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    ax1.set_title(f\"Top {len(top3)} Movies for {director_name} — Rating & Runtime\")\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Save cleaned datasets (full and sampled) to Kaggle working directory\n",
    "full_path = \"/kaggle/working/imdb_cleaned_full.csv\"\n",
    "sample_path = \"/kaggle/working/imdb_cleaned_sample.csv\"\n",
    "\n",
    "# Ensure df and df_sample exist (they should from preprocessing cells)\n",
    "if 'df' in globals():\n",
    "    df.to_csv(full_path, index=False)\n",
    "    print(f\"Saved cleaned full dataset to: {full_path}\")\n",
    "else:\n",
    "    print(\"Warning: 'df' not found, full dataset not saved.\")\n",
    "\n",
    "if 'df_sample' in globals():\n",
    "    df_sample.to_csv(sample_path, index=False)\n",
    "    print(f\"Saved cleaned sample dataset to: {sample_path}\")\n",
    "else:\n",
    "    print(\"Warning: 'df_sample' not found, sample dataset not saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
